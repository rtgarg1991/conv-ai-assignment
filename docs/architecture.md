```mermaid
flowchart LR
  %% Styling
  classDef ingestion fill:#e3f2fd,stroke:#1976d2,color:#0d47a1
  classDef indexing fill:#e0f2f1,stroke:#00897b,color:#004d40
  classDef retrieval fill:#e8f5e9,stroke:#43a047,color:#1b5e20
  classDef generation fill:#fff3e0,stroke:#fb8c00,color:#e65100
  classDef evaluation fill:#f3e5f5,stroke:#8e24aa,color:#4a148c
  classDef ui fill:#fce4ec,stroke:#d81b60,color:#880e4f

  subgraph "SECTION 1 - DATA INGESTION"
    A["ğŸ”— Fixed URLs (200)"]:::ingestion
    B["ğŸ² Random URLs (300)"]:::ingestion
    A --> D["âš™ï¸ URL Loader"]:::ingestion
    B --> D
    D --> E["ğŸ•·ï¸ Web Scraper"]:::ingestion
    E --> F["âœ‚ï¸ Cleaner + Chunker<br/>(200-400 tokens, 50 overlap)"]:::ingestion
    F --> G["ğŸ“¦ Corpus<br/>(chunks + metadata)"]:::ingestion
  end

  subgraph "SECTION 2 - INDEXING"
    G --> H["ğŸ§  Dense Embeddings<br/>(SentenceTransformer)"]:::indexing
    G --> I["ğŸ“ Sparse Tokens<br/>(BM25)"]:::indexing
    H --> J["ğŸ—„ï¸ FAISS Vector Index"]:::indexing
    I --> K["ğŸ—‚ï¸ BM25 Index"]:::indexing
  end

  subgraph "SECTION 3 - RETRIEVAL"
    L["â“ User Query"]:::retrieval
    L --> M["ğŸ” Dense Retrieval (Top-K)"]:::retrieval
    L --> N["ğŸ” Sparse Retrieval (Top-K)"]:::retrieval
    J -.-> M
    K -.-> N
    M --> O["âš–ï¸ RRF Fusion (k=60)"]:::retrieval
    N --> O
    O --> P["ğŸ“„ Top-N Context Chunks"]:::retrieval
  end

  subgraph "SECTION 4 - GENERATION"
    P --> Q["ğŸ“‹ Prompt Builder"]:::generation
    Q --> R["ğŸ¤– LLM<br/>(Flan-T5 / GPT2)"]:::generation
    R --> S["ğŸ’¬ Generated Answer"]:::generation
  end

  subgraph "SECTION 5 - EVALUATION"
    G --> T["â“ Q&A Generator<br/>(100 Qs)"]:::evaluation
    T --> U["ğŸ§ª Evaluation Runner"]:::evaluation
    U --> V["ğŸ“Š Metrics<br/>MRR, ROUGE-L, BERTScore"]:::evaluation
    U --> W["ğŸ”¬ Ablation +<br/>Error Analysis"]:::evaluation
    V --> X["ğŸ“‘ HTML Report"]:::evaluation
    W --> X
  end

  subgraph "SECTION 6 - UI"
    S --> Y["ğŸ–¥ï¸ Streamlit App"]:::ui
    P --> Y
    O --> Y
  end
```

## Architecture Overview

### Section 1: Data Ingestion (Blue)
- **Fixed URLs (200)**: Curated Wikipedia articles for consistent evaluation
- **Random URLs (300)**: Randomly sampled for diversity
- **URL Loader â†’ Scraper â†’ Chunker**: Full ETL pipeline
- **Output**: Corpus with 200-400 token chunks, 50-token overlap

### Section 2: Indexing (Teal)
- **Dense Embeddings**: SentenceTransformer (all-MiniLM-L6-v2) encodes chunks
- **Sparse Tokens**: BM25 tokenizes for keyword matching
- **Dual Index**: FAISS for vectors, BM25 for keywords

### Section 3: Retrieval (Green)
- **Parallel Retrieval**: Query hits both dense and sparse indices
- **RRF Fusion**: Combines rankings with k=60
- **Output**: Top-10 context chunks

### Section 4: Generation (Orange)
- **Prompt Builder**: Formats context + query
- **LLM**: Flan-T5-base generates answer
- **Output**: Natural language response

### Section 5: Evaluation (Purple)
- **Q&A Generator**: Creates 100 diverse questions
- **Metrics**: MRR (0.8587), ROUGE-L (0.2458), BERTScore (0.7019)
- **Analysis**: Ablation studies + error categorization
- **Output**: HTML report with charts

### Section 6: UI (Pink)
- **Streamlit App**: Displays query, answer, context, and scores

![Architecture Diagram](/Users/rohitgarg/Work/conv ai assignment/data/architecture_diagram.png)
